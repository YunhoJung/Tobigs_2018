{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "os.getcwd()\n",
    "path = \"/Users/yunho/projects/Tobigs_2018\"\n",
    "\n",
    "doc = pd.read_csv(os.path.join(path,\"white.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>x3</th>\n",
       "      <th>x4</th>\n",
       "      <th>x5</th>\n",
       "      <th>x6</th>\n",
       "      <th>x7</th>\n",
       "      <th>x8</th>\n",
       "      <th>x9</th>\n",
       "      <th>x10</th>\n",
       "      <th>x11</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.0</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.36</td>\n",
       "      <td>20.7</td>\n",
       "      <td>0.045</td>\n",
       "      <td>45.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>1.0010</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.45</td>\n",
       "      <td>8.8</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.3</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.34</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.049</td>\n",
       "      <td>14.0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>0.9940</td>\n",
       "      <td>3.30</td>\n",
       "      <td>0.49</td>\n",
       "      <td>9.5</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.1</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.40</td>\n",
       "      <td>6.9</td>\n",
       "      <td>0.050</td>\n",
       "      <td>30.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>0.9951</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.44</td>\n",
       "      <td>10.1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.2</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.32</td>\n",
       "      <td>8.5</td>\n",
       "      <td>0.058</td>\n",
       "      <td>47.0</td>\n",
       "      <td>186.0</td>\n",
       "      <td>0.9956</td>\n",
       "      <td>3.19</td>\n",
       "      <td>0.40</td>\n",
       "      <td>9.9</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.2</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.32</td>\n",
       "      <td>8.5</td>\n",
       "      <td>0.058</td>\n",
       "      <td>47.0</td>\n",
       "      <td>186.0</td>\n",
       "      <td>0.9956</td>\n",
       "      <td>3.19</td>\n",
       "      <td>0.40</td>\n",
       "      <td>9.9</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8.1</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.40</td>\n",
       "      <td>6.9</td>\n",
       "      <td>0.050</td>\n",
       "      <td>30.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>0.9951</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.44</td>\n",
       "      <td>10.1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    x1    x2    x3    x4     x5    x6     x7      x8    x9   x10   x11  y\n",
       "0  7.0  0.27  0.36  20.7  0.045  45.0  170.0  1.0010  3.00  0.45   8.8  6\n",
       "1  6.3  0.30  0.34   1.6  0.049  14.0  132.0  0.9940  3.30  0.49   9.5  6\n",
       "2  8.1  0.28  0.40   6.9  0.050  30.0   97.0  0.9951  3.26  0.44  10.1  6\n",
       "3  7.2  0.23  0.32   8.5  0.058  47.0  186.0  0.9956  3.19  0.40   9.9  6\n",
       "4  7.2  0.23  0.32   8.5  0.058  47.0  186.0  0.9956  3.19  0.40   9.9  6\n",
       "5  8.1  0.28  0.40   6.9  0.050  30.0   97.0  0.9951  3.26  0.44  10.1  6"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# EDA\n",
    "doc.head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(doc.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data type of y -> str\n",
    "doc.y= doc.y.astype(\"str\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6    2198\n",
       "5    1457\n",
       "7     880\n",
       "8     175\n",
       "4     163\n",
       "3      20\n",
       "9       5\n",
       "Name: y, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check y distribution\n",
    "doc.y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4898, 12)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shape\n",
    "doc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split X, y\n",
    "X_data=doc.iloc[:,0:11]\n",
    "y_data = doc.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 1., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# One-hot encoding for y\n",
    "ohe = OneHotEncoder()\n",
    "y_data1 = y_data.values\n",
    "y_data1 = ohe.fit_transform(y_data1.reshape(-1, 1)).toarray()\n",
    "y_data1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data partitioning\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_data, y_data1, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 \t 1.9452848\n",
      "1000 \t 1.5538563\n",
      "2000 \t 1.5393864\n",
      "3000 \t 1.5364162\n",
      "4000 \t 1.5353813\n",
      "5000 \t 1.5346875\n",
      "6000 \t 1.5341562\n",
      "7000 \t 1.5337626\n",
      "8000 \t 1.5334461\n",
      "9000 \t 1.5331813\n",
      "10000 \t 1.5329592\n"
     ]
    }
   ],
   "source": [
    "# 1.  sigmoid, GradientDescentOptimizer \n",
    " \n",
    "X = tf.placeholder(tf.float32,[None, 11])\n",
    "Y = tf.placeholder(tf.float32,[None, 7])\n",
    " \n",
    "# init\n",
    "W1 = tf.Variable(tf.random_normal([11,7],stddev=0.01))\n",
    "b1 = tf.Variable(tf.zeros([7]))\n",
    "hypothesis = tf.sigmoid(tf.matmul(X, W1) + b1)\n",
    " \n",
    "# cross entropy cost function\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=hypothesis, labels=Y))\n",
    "# GradientDescentOptimizer\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    " \n",
    "# predicted value H(X) > 0.5 이면  true, otherwise false\n",
    "predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
    "\n",
    "# calculate the average value of 0 or 1 using the number of epochs\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))\n",
    " \n",
    "# train\n",
    "init = tf.global_variables_initializer()\n",
    "sess= tf.Session()\n",
    "sess.run(init)\n",
    " \n",
    "# 10000 epochs\n",
    "for step in range(10001):\n",
    "    cost_val,  _ = sess.run([cost, optimizer], feed_dict={X:X_train, Y:Y_train})\n",
    "    if step % 1000 == 0:\n",
    "        print(step, '\\t', cost_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy :  0.27959183\n"
     ]
    }
   ],
   "source": [
    "# accuracy 27.9%\n",
    "predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
    "is_correct = tf.equal(tf.argmax(predicted,1), tf.argmax(Y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct,tf.float32))\n",
    "print('accuracy : ', sess.run(accuracy, feed_dict={X: X_test, Y: Y_test}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 \t 1.9501402\n",
      "1000 \t 1.2856228\n",
      "2000 \t 1.2856228\n",
      "3000 \t 1.2856243\n",
      "4000 \t 1.2856834\n",
      "5000 \t 1.285623\n",
      "6000 \t 1.2856226\n",
      "7000 \t 1.2856196\n",
      "8000 \t 1.285624\n",
      "9000 \t 1.2856238\n",
      "10000 \t 1.2856265\n"
     ]
    }
   ],
   "source": [
    "# 2.  Multi-layer neural network, sigmoid, AdamOptimizer \n",
    " \n",
    "X = tf.placeholder(tf.float32,[None, 11])\n",
    "Y = tf.placeholder(tf.float32,[None, 7])\n",
    " \n",
    "# 3-layer neural network\n",
    "W1 = tf.Variable(tf.random_normal([11,10],stddev=0.01))\n",
    "b1 = tf.Variable(tf.zeros([10]))\n",
    "h1 = tf.sigmoid(tf.matmul(X, W1) + b1)\n",
    " \n",
    "W2 = tf.Variable(tf.random_normal([10,10],stddev=0.01))\n",
    "b2 = tf.Variable(tf.zeros([10]))\n",
    "h2 = tf.sigmoid(tf.matmul(h1, W2) + b2)\n",
    " \n",
    "W3 = tf.Variable(tf.random_normal([10,7],stddev=0.01))\n",
    "b3 = tf.Variable(tf.zeros([7]))\n",
    "hypothesis = tf.matmul(h2, W3) + b3\n",
    " \n",
    "# cross entropy cost function\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=hypothesis, labels=Y))\n",
    "# Adam\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.1).minimize(cost)\n",
    " \n",
    "# predicted value H(X) > 0.5 is true, else false\n",
    "predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
    "\n",
    "# calculate the average value of 0 or 1 using the number of epochs\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))\n",
    " \n",
    "# train\n",
    "init = tf.global_variables_initializer()\n",
    "sess= tf.Session()\n",
    "sess.run(init)\n",
    " \n",
    "# 10000 epochs\n",
    "for step in range(10001):\n",
    "    cost_val,  _ = sess.run([cost, optimizer], feed_dict={X:X_train, Y:Y_train})\n",
    "    if step % 1000 == 0:\n",
    "        print(step, '\\t', cost_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy :  0.27959183\n"
     ]
    }
   ],
   "source": [
    "predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
    "is_correct = tf.equal(tf.argmax(predicted,1), tf.argmax(Y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct,tf.float32))\n",
    "print('accuracy : ', sess.run(accuracy, feed_dict={X: X_test, Y: Y_test}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "0 \t 64.909874\n",
      "1000 \t 1.2856234\n",
      "2000 \t 1.2856234\n",
      "3000 \t 1.2856228\n",
      "4000 \t 1.2856228\n",
      "5000 \t 1.2856228\n",
      "6000 \t 1.2856236\n",
      "7000 \t 1.2856209\n",
      "8000 \t 1.2856208\n",
      "9000 \t 1.2856185\n",
      "10000 \t 1.2856218\n"
     ]
    }
   ],
   "source": [
    "#3. Xavier algorithm for weight initialization, Multi-layer neural network, Adamoptimizer, relu \n",
    " \n",
    "tf.reset_default_graph()\n",
    " \n",
    "# 3-layer neural network\n",
    "X = tf.placeholder(tf.float32,[None, 11])\n",
    "Y = tf.placeholder(tf.float32,[None, 7])\n",
    " \n",
    "W1 = tf.get_variable(\"W1\",shape = [11,10],initializer = tf.contrib.layers.xavier_initializer())\n",
    "b1 = tf.Variable(tf.zeros([10]))\n",
    "h1 = tf.nn.relu(tf.matmul(X, W1) + b1)\n",
    " \n",
    "W2 = tf.get_variable(\"W2\",shape = [10,10],initializer = tf.contrib.layers.xavier_initializer())\n",
    "b2 = tf.Variable(tf.zeros([10]))\n",
    "h2 = tf.nn.relu(tf.matmul(h1, W2) + b2)\n",
    " \n",
    "W3 = tf.get_variable(\"W3\",shape = [10,7] ,initializer = tf.contrib.layers.xavier_initializer())\n",
    "b3 = tf.Variable(tf.zeros([7]))\n",
    "hypothesis = tf.matmul(h2, W3) + b3\n",
    " \n",
    "# cross entropy cost function \n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=hypothesis, labels=Y))\n",
    "# Adam\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.1).minimize(cost)\n",
    " \n",
    "# predicted value H(X) > 0.5 is true, else false\n",
    "predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
    "# calculate the average value of 0 or 1 using the number of epochs\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))\n",
    " \n",
    "# train\n",
    "init = tf.global_variables_initializer()\n",
    "sess= tf.Session()\n",
    "sess.run(init)\n",
    " \n",
    "# 10000 epochs\n",
    "for step in range(10001):\n",
    "    cost_val,  _ = sess.run([cost, optimizer], feed_dict={X:X_train, Y:Y_train})\n",
    "    if step % 1000 == 0:\n",
    "        print(step, '\\t', cost_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy :  0.27959183\n"
     ]
    }
   ],
   "source": [
    "predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
    "is_correct = tf.equal(tf.argmax(predicted,1), tf.argmax(Y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct,tf.float32))\n",
    "print('accuracy : ', sess.run(accuracy, feed_dict={X: X_test, Y: Y_test}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-22-907088538999>:11: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "0 \t 1.912752\n",
      "1000 \t 1.2870835\n",
      "2000 \t 1.2867362\n",
      "3000 \t 1.286476\n",
      "4000 \t 1.2850051\n",
      "5000 \t 1.2809805\n",
      "6000 \t 1.2627347\n",
      "7000 \t 1.2429771\n",
      "8000 \t 1.2425663\n",
      "9000 \t 1.2446607\n",
      "10000 \t 1.2338846\n"
     ]
    }
   ],
   "source": [
    "# 4. Dropout, sigmoid, GradientDescentOptimizer, Multi-layer neural network\n",
    " \n",
    "X = tf.placeholder(tf.float32,[None, 11])\n",
    "Y = tf.placeholder(tf.float32,[None, 7])\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    " \n",
    "# 3-layer neural network\n",
    "W1 = tf.Variable(tf.random_normal([11,30],stddev=0.01))\n",
    "b1 = tf.Variable(tf.zeros([30]))\n",
    "h1 = tf.sigmoid(tf.matmul(X, W1) + b1)\n",
    "h1 = tf.nn.dropout(h1, keep_prob)\n",
    " \n",
    "W2 = tf.Variable(tf.random_normal([30,256],stddev=0.01))\n",
    "b2 = tf.Variable(tf.zeros([256]))\n",
    "h2 = tf.sigmoid(tf.matmul(h1, W2) + b2)\n",
    "h2 = tf.nn.dropout(h2, keep_prob)\n",
    " \n",
    "W3 = tf.Variable(tf.random_normal([256,7],stddev=0.01))\n",
    "b3 = tf.Variable(tf.zeros([7]))\n",
    "hypothesis = tf.matmul(h2, W3) + b3\n",
    " \n",
    "# cross entropy cost function\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=hypothesis, labels=Y))\n",
    "# GradientDescentOptimizer\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    " \n",
    "# train\n",
    "init = tf.global_variables_initializer()\n",
    "sess= tf.Session()\n",
    "sess.run(init)\n",
    " \n",
    "# 10000 epochs - dropout 0.8 \n",
    "for step in range(10001):\n",
    "    cost_val,  _ = sess.run([cost, optimizer], feed_dict={X:X_train, Y:Y_train,keep_prob:0.8})\n",
    "    if step % 1000 == 0:\n",
    "        print(step, '\\t', cost_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy :  0.27959183\n"
     ]
    }
   ],
   "source": [
    "predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
    "is_correct = tf.equal(tf.argmax(predicted,1), tf.argmax(Y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct,tf.float32))\n",
    "# No dropout for test\n",
    "print('accuracy : ', sess.run(accuracy, feed_dict={X: X_test, Y: Y_test,keep_prob:1}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 \t 22.76609\n",
      "1000 \t 1.2856231\n",
      "2000 \t 1.2856231\n",
      "3000 \t 1.2856228\n",
      "4000 \t 1.2856234\n",
      "5000 \t 1.2856231\n",
      "6000 \t 1.2856194\n",
      "7000 \t 1.2856222\n",
      "8000 \t 1.2856232\n",
      "9000 \t 1.2856221\n",
      "10000 \t 1.2856295\n"
     ]
    }
   ],
   "source": [
    "#5. Dropout, relu, Multi-layer neural network, AdamOptimizer, Xavier algorithm for weight initialization\n",
    " \n",
    "tf.reset_default_graph()\n",
    " \n",
    "# 4-layer neural network\n",
    "X = tf.placeholder(tf.float32,[None, 11])\n",
    "Y = tf.placeholder(tf.float32,[None, 7])\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    " \n",
    "# Xavier algorithm for weight initialization\n",
    "W1 = tf.get_variable(\"W1\",shape = [11,20],initializer = tf.contrib.layers.xavier_initializer())\n",
    "# bias\n",
    "b1 = tf.Variable(tf.zeros([20]))\n",
    "# relu \n",
    "h1 = tf.nn.relu(tf.matmul(X, W1) + b1)\n",
    "# dropout\n",
    "h1 = tf.nn.dropout(h1, keep_prob)\n",
    " \n",
    "\n",
    "# Xavier algorithm for weight initialization \n",
    "W2 = tf.get_variable(\"W2\",shape = [20,80],initializer = tf.contrib.layers.xavier_initializer())\n",
    "b2 = tf.Variable(tf.zeros([80]))\n",
    "h2 = tf.nn.relu(tf.matmul(h1, W2) + b2)\n",
    "h2 = tf.nn.dropout(h2, keep_prob)\n",
    " \n",
    "W3 = tf.get_variable(\"W3\",shape = [80,30] ,initializer = tf.contrib.layers.xavier_initializer())\n",
    "b3 = tf.Variable(tf.zeros([30]))\n",
    "h3 = tf.nn.relu(tf.matmul(h2, W3) + b3)\n",
    "h3 = tf.nn.dropout(h3, keep_prob)\n",
    " \n",
    "W4 = tf.get_variable(\"W4\",shape = [30,7] ,initializer = tf.contrib.layers.xavier_initializer())\n",
    "b4 = tf.Variable(tf.zeros([7]))\n",
    "h4 = tf.nn.dropout(h3, keep_prob)\n",
    " \n",
    "hypothesis = tf.matmul(h4, W4) + b4\n",
    " \n",
    "# cross entropy cost function\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=hypothesis, labels=Y))\n",
    "# AdamOptimizer\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.1).minimize(cost)\n",
    " \n",
    "# train\n",
    "init = tf.global_variables_initializer()\n",
    "sess= tf.Session()\n",
    "sess.run(init)\n",
    " \n",
    "# 10000 epochs - dropout 0.8 \n",
    "for step in range(10001):\n",
    "    cost_val,  _ = sess.run([cost, optimizer], feed_dict={X:X_train, Y:Y_train,keep_prob:0.8})\n",
    "    if step % 1000 == 0:\n",
    "        print(step, '\\t', cost_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy :  0.27959183\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
    "is_correct = tf.equal(tf.argmax(predicted,1), tf.argmax(Y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct,tf.float32))\n",
    "print('accuracy : ', sess.run(accuracy, feed_dict={X: X_test, Y: Y_test,keep_prob:1}))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
