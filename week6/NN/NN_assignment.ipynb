{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-1-9bca1650e536>:6: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /Users/yunho/.pyenv/versions/3.7.4/envs/Tobigs_2018-env/lib/python3.7/site-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /Users/yunho/.pyenv/versions/3.7.4/envs/Tobigs_2018-env/lib/python3.7/site-packages/tensorflow_core/contrib/learn/python/learn/datasets/base.py:252: _internal_retry.<locals>.wrap.<locals>.wrapped_fn (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use urllib or similar directly.\n",
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "WARNING:tensorflow:From /Users/yunho/.pyenv/versions/3.7.4/envs/Tobigs_2018-env/lib/python3.7/site-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting ./mnist/data/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "WARNING:tensorflow:From /Users/yunho/.pyenv/versions/3.7.4/envs/Tobigs_2018-env/lib/python3.7/site-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting ./mnist/data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /Users/yunho/.pyenv/versions/3.7.4/envs/Tobigs_2018-env/lib/python3.7/site-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting ./mnist/data/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting ./mnist/data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /Users/yunho/.pyenv/versions/3.7.4/envs/Tobigs_2018-env/lib/python3.7/site-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "# Set Up & Data Load\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "mnist = input_data.read_data_sets(\"./mnist/data/\",one_hot=True) \n",
    "\n",
    "## Get 100 data from train dataset (img, label) tuple pair\n",
    "## train_images, train_labels = mnist.train.next_batch(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래 부터 빈칸을 채워가면서 진행하세요:) 수정하고 싶은 부분은 자유롭게 수정하셔도 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = mnist.train.images\n",
    "X_val = mnist.validation.images\n",
    "X_test = mnist.test.images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = mnist.train.labels\n",
    "y_val = mnist.validation.labels\n",
    "y_test = mnist.test.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape :  (55000, 784)\n",
      "X_val shape :  (5000, 784)\n",
      "X_test shape :  (10000, 784)\n"
     ]
    }
   ],
   "source": [
    "print(\"X_train shape : \", X_train.shape)\n",
    "print(\"X_val shape : \", X_val.shape)\n",
    "print(\"X_test shape : \", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train shape :  (55000, 10)\n",
      "y_val shape :  (5000, 10)\n",
      "y_test shape :  (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "print(\"y_train shape : \", y_train.shape)\n",
    "print(\"y_val shape : \", y_val.shape)\n",
    "print(\"y_test shape : \", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 1., 0., 0.],\n",
       "       [0., 0., 1., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 3-layer, relu, AdamOptimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "Y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "# hidden layer 1\n",
    "W1 = tf.Variable(tf.random_normal([784, 256]))\n",
    "b1 = tf.Variable(tf.random_normal([256]))\n",
    "L1 = tf.nn.relu(tf.matmul(X, W1) + b1)\n",
    "\n",
    "# hidden layer 2\n",
    "W2 = tf.Variable(tf.random_normal([256, 128]))\n",
    "b2 = tf.Variable(tf.random_normal([128]))\n",
    "L2 = tf.nn.relu(tf.matmul(L1, W2) + b2)\n",
    "\n",
    "# hidden layer 3\n",
    "W3 = tf.Variable(tf.random_normal([128, 64]))\n",
    "b3 = tf.Variable(tf.random_normal([64]))\n",
    "L3 = tf.nn.relu(tf.matmul(L2, W3) + b3)\n",
    "\n",
    "# output layer\n",
    "W4 = tf.Variable(tf.random_normal([64, 10]))\n",
    "b4 = tf.Variable(tf.random_normal([10]))\n",
    "hypothesis = tf.matmul(L3, W4) + b4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits = hypothesis, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 train acc: 0.45023635 val acc: 0.4516\n",
      "epoch: 1 train acc: 0.65181816 val acc: 0.6568\n",
      "epoch: 2 train acc: 0.7298727 val acc: 0.7382\n",
      "epoch: 3 train acc: 0.77387273 val acc: 0.777\n",
      "epoch: 4 train acc: 0.80185455 val acc: 0.7986\n",
      "epoch: 5 train acc: 0.8202182 val acc: 0.8172\n",
      "epoch: 6 train acc: 0.83532727 val acc: 0.8324\n",
      "epoch: 7 train acc: 0.84743637 val acc: 0.8414\n",
      "epoch: 8 train acc: 0.8575818 val acc: 0.8494\n",
      "epoch: 9 train acc: 0.8667273 val acc: 0.8604\n",
      "epoch: 10 train acc: 0.8739091 val acc: 0.8668\n",
      "epoch: 11 train acc: 0.8802909 val acc: 0.872\n",
      "epoch: 12 train acc: 0.88567275 val acc: 0.8754\n",
      "epoch: 13 train acc: 0.8902 val acc: 0.8794\n",
      "epoch: 14 train acc: 0.89494544 val acc: 0.8814\n",
      "epoch: 15 train acc: 0.89967275 val acc: 0.8844\n",
      "epoch: 16 train acc: 0.90276366 val acc: 0.8862\n",
      "epoch: 17 train acc: 0.9058909 val acc: 0.8886\n",
      "epoch: 18 train acc: 0.90927273 val acc: 0.8914\n",
      "epoch: 19 train acc: 0.91245455 val acc: 0.8918\n",
      "epoch: 20 train acc: 0.91563636 val acc: 0.8934\n",
      "epoch: 21 train acc: 0.91816366 val acc: 0.8956\n",
      "epoch: 22 train acc: 0.9214909 val acc: 0.898\n",
      "epoch: 23 train acc: 0.9238909 val acc: 0.9\n",
      "epoch: 24 train acc: 0.92547274 val acc: 0.901\n",
      "epoch: 25 train acc: 0.9279091 val acc: 0.9024\n",
      "epoch: 26 train acc: 0.93005455 val acc: 0.9032\n",
      "epoch: 27 train acc: 0.9322182 val acc: 0.9022\n",
      "epoch: 28 train acc: 0.93472725 val acc: 0.9058\n",
      "epoch: 29 train acc: 0.9348364 val acc: 0.9058\n",
      "[Test Accuracy] : 0.8974\n"
     ]
    }
   ],
   "source": [
    "# initialize\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# train hyperparameters\n",
    "epoch_cnt = 30\n",
    "batch_size = 1000\n",
    "iteration = len(X_train) // batch_size\n",
    "\n",
    "# train\n",
    "with tf.Session() as sess:\n",
    "    # run the initializer\n",
    "    sess.run(init)\n",
    "    pre_train_acc = 0.0\n",
    "    max_val_acc = 0.0\n",
    "    \n",
    "    for epoch in range(epoch_cnt):\n",
    "        avg_loss = 0.\n",
    "        start = 0\n",
    "        end = batch_size\n",
    "        \n",
    "        for i in range(iteration):\n",
    "            _, loss = sess.run([optimizer, cost],\n",
    "                              feed_dict={X:X_train[start:end], Y:y_train[start:end]})\n",
    "            start += batch_size\n",
    "            end += batch_size\n",
    "            \n",
    "            # compute average loss\n",
    "            avg_loss += loss/iteration\n",
    "        \n",
    "        # validate model\n",
    "        preds = tf.nn.softmax(hypothesis) # apply softmax to logits\n",
    "        correct_prediction = tf.equal(tf.argmax(preds, 1), tf.argmax(Y, 1))\n",
    "        \n",
    "        # calculate accuracy\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "        cur_train_acc = accuracy.eval({X:X_train, Y:y_train})\n",
    "        cur_val_acc = accuracy.eval({X:X_val, Y:y_val})\n",
    "        \n",
    "        print(\"epoch:\", epoch, \"train acc:\", cur_train_acc, \"val acc:\", cur_val_acc)\n",
    "        \n",
    "    # test model\n",
    "    preds = tf.nn.softmax(hypothesis)\n",
    "    correct_prediction = tf.equal(tf.argmax(preds, 1), tf.argmax(Y, 1))\n",
    "    \n",
    "    # calculate accuracy\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "    print(\"[Test Accuracy] :\", accuracy.eval({X:X_test, Y:y_test}))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: [5 5 5 ... 6 5 8]\n",
      "Y: [7 2 1 ... 4 5 6]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    # run the initializer\n",
    "    sess.run(init)\n",
    "    model = tf.nn.softmax(hypothesis)\n",
    "    prediction = tf.argmax(model, 1)\n",
    "    target = tf.argmax(Y, 1)\n",
    "    \n",
    "    print(\"Prediction:\", sess.run(prediction, feed_dict={X:X_test}))\n",
    "    print(\"Y:\", sess.run(target, feed_dict={Y:y_test}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 3-layer, relu, AdamOpimizer, Xavier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "Y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "# hidden layer 1\n",
    "W1_ = tf.get_variable(\"W1_\", shape=[784, 256], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b1 = tf.Variable(tf.random_normal([256]))\n",
    "L1 = tf.nn.relu(tf.matmul(X, W1_) + b1)\n",
    "\n",
    "# hidden layer 2\n",
    "W2_ = tf.get_variable(\"W2_\", shape=[256, 256], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b2 = tf.Variable(tf.random_normal([256]))\n",
    "L2 = tf.nn.relu(tf.matmul(L1, W2_) + b2)\n",
    "\n",
    "# hidden layer 3\n",
    "W3_ = tf.get_variable(\"W3_\", shape=[256, 256], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b3 = tf.Variable(tf.random_normal([256]))\n",
    "L3 = tf.nn.relu(tf.matmul(L2, W3_) + b3)\n",
    "\n",
    "# output layer\n",
    "W4_ = tf.get_variable(\"W4_\", shape=[256, 10], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b4 = tf.Variable(tf.random_normal([10]))\n",
    "hypothesis = tf.matmul(L3, W4_) + b4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits = hypothesis, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 train acc: 0.93834543 val acc: 0.9398\n",
      "epoch: 1 train acc: 0.9604545 val acc: 0.958\n",
      "epoch: 2 train acc: 0.97081816 val acc: 0.9656\n",
      "epoch: 3 train acc: 0.97229093 val acc: 0.9632\n",
      "epoch: 4 train acc: 0.9772909 val acc: 0.9674\n",
      "epoch: 5 train acc: 0.97892725 val acc: 0.9662\n",
      "epoch: 6 train acc: 0.97949094 val acc: 0.9674\n",
      "epoch: 7 train acc: 0.97294545 val acc: 0.9628\n",
      "epoch: 8 train acc: 0.9804182 val acc: 0.9696\n",
      "epoch: 9 train acc: 0.97734547 val acc: 0.9644\n",
      "epoch: 10 train acc: 0.9838182 val acc: 0.9682\n",
      "epoch: 11 train acc: 0.98596364 val acc: 0.9684\n",
      "epoch: 12 train acc: 0.9886909 val acc: 0.9732\n",
      "epoch: 13 train acc: 0.98725456 val acc: 0.9706\n",
      "epoch: 14 train acc: 0.9837273 val acc: 0.969\n",
      "epoch: 15 train acc: 0.98983634 val acc: 0.9732\n",
      "epoch: 16 train acc: 0.98612726 val acc: 0.9694\n",
      "epoch: 17 train acc: 0.9875818 val acc: 0.9718\n",
      "epoch: 18 train acc: 0.9894182 val acc: 0.9726\n",
      "epoch: 19 train acc: 0.99365455 val acc: 0.9746\n",
      "epoch: 20 train acc: 0.99276364 val acc: 0.974\n",
      "epoch: 21 train acc: 0.9923273 val acc: 0.9716\n",
      "epoch: 22 train acc: 0.99512726 val acc: 0.9738\n",
      "epoch: 23 train acc: 0.99243635 val acc: 0.9726\n",
      "epoch: 24 train acc: 0.99236363 val acc: 0.972\n",
      "epoch: 25 train acc: 0.9820909 val acc: 0.966\n",
      "epoch: 26 train acc: 0.9917091 val acc: 0.9702\n",
      "epoch: 27 train acc: 0.99256366 val acc: 0.972\n",
      "epoch: 28 train acc: 0.9944909 val acc: 0.9728\n",
      "epoch: 29 train acc: 0.9947091 val acc: 0.9766\n",
      "[Test Accuracy] : 0.9749\n"
     ]
    }
   ],
   "source": [
    "# initialize\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# train hyperparameters\n",
    "epoch_cnt = 30\n",
    "batch_size = 1000\n",
    "iteration = len(X_train) // batch_size\n",
    "\n",
    "# train\n",
    "with tf.Session() as sess:\n",
    "    # run the initializer\n",
    "    sess.run(init)\n",
    "    pre_train_acc = 0.0\n",
    "    max_val_acc = 0.0\n",
    "    \n",
    "    for epoch in range(epoch_cnt):\n",
    "        avg_loss = 0.\n",
    "        start = 0\n",
    "        end = batch_size\n",
    "        \n",
    "        for i in range(iteration):\n",
    "            _, loss = sess.run([optimizer, cost],\n",
    "                              feed_dict={X:X_train[start:end], Y:y_train[start:end]})\n",
    "            start += batch_size\n",
    "            end += batch_size\n",
    "            \n",
    "            # compute average loss\n",
    "            avg_loss += loss/iteration\n",
    "        \n",
    "        # validate model\n",
    "        preds = tf.nn.softmax(hypothesis) # apply softmax to logits\n",
    "        correct_prediction = tf.equal(tf.argmax(preds, 1), tf.argmax(Y, 1))\n",
    "        \n",
    "        # calculate accuracy\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "        cur_train_acc = accuracy.eval({X:X_train, Y:y_train})\n",
    "        cur_val_acc = accuracy.eval({X:X_val, Y:y_val})\n",
    "        \n",
    "        print(\"epoch:\", epoch, \"train acc:\", cur_train_acc, \"val acc:\", cur_val_acc)\n",
    "        \n",
    "    # test model\n",
    "    preds = tf.nn.softmax(hypothesis)\n",
    "    correct_prediction = tf.equal(tf.argmax(preds, 1), tf.argmax(Y, 1))\n",
    "    \n",
    "    # calculate accuracy\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "    print(\"[Test Accuracy] :\", accuracy.eval({X:X_test, Y:y_test}))   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Looks going overfitting -> dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 4-layer, relu, AdamOpimizer, Xavier, dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "Y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "# hidden layer 1\n",
    "W1_ = tf.get_variable(\"W1_\", shape=[784, 256], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b1_ = tf.get_variable(\"b1_\", [256], initializer=tf.zeros_initializer())\n",
    "L1 = tf.nn.relu(tf.matmul(X, W1_) + b1_)\n",
    "L1 = tf.nn.dropout(L1, keep_prob=keep_prob)\n",
    "\n",
    "# hidden layer 2\n",
    "W2_ = tf.get_variable(\"W2_\", shape=[256, 128], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b2_ = tf.get_variable(\"b2_\", [128], initializer=tf.zeros_initializer())\n",
    "L2 = tf.nn.relu(tf.matmul(L1, W2_) + b2_)\n",
    "L2 = tf.nn.dropout(L2, keep_prob=keep_prob)\n",
    "\n",
    "# hidden layer 3\n",
    "W3_ = tf.get_variable(\"W3_\", shape=[128, 64], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b3_ = tf.get_variable(\"b3_\", [64], initializer=tf.zeros_initializer())\n",
    "L3 = tf.nn.relu(tf.matmul(L2, W3_) + b3_)\n",
    "L3 = tf.nn.dropout(L3, keep_prob=keep_prob)\n",
    "\n",
    "# hidden layer 4\n",
    "W4_ = tf.get_variable(\"W4_\", shape=[64, 32], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b4_ = tf.get_variable(\"b4_\", [32], initializer=tf.zeros_initializer())\n",
    "L4 = tf.nn.relu(tf.matmul(L3, W4_) + b4_)\n",
    "L4 = tf.nn.dropout(L4, keep_prob=keep_prob)\n",
    "\n",
    "# output layer\n",
    "W5_ = tf.get_variable(\"W5_\", shape=[32, 10], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b5_ = tf.get_variable(\"b5_\", [10], initializer=tf.zeros_initializer())\n",
    "hypothesis = tf.matmul(L4, W5_) + b5_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits = hypothesis, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 train acc: 0.9322364 val acc: 0.939\n",
      "epoch: 1 train acc: 0.95483637 val acc: 0.9554\n",
      "epoch: 2 train acc: 0.96410906 val acc: 0.964\n",
      "epoch: 3 train acc: 0.97007275 val acc: 0.9682\n",
      "epoch: 4 train acc: 0.9715091 val acc: 0.9664\n",
      "epoch: 5 train acc: 0.9757818 val acc: 0.9708\n",
      "epoch: 6 train acc: 0.97778183 val acc: 0.9696\n",
      "epoch: 7 train acc: 0.97907275 val acc: 0.9706\n",
      "epoch: 8 train acc: 0.98154545 val acc: 0.9736\n",
      "epoch: 9 train acc: 0.9828727 val acc: 0.9736\n",
      "epoch: 10 train acc: 0.9845273 val acc: 0.974\n",
      "epoch: 11 train acc: 0.98483634 val acc: 0.9736\n",
      "epoch: 12 train acc: 0.9846727 val acc: 0.9752\n",
      "epoch: 13 train acc: 0.9851091 val acc: 0.9758\n",
      "epoch: 14 train acc: 0.9861091 val acc: 0.9764\n",
      "epoch: 15 train acc: 0.98694545 val acc: 0.9782\n",
      "epoch: 16 train acc: 0.98645455 val acc: 0.9762\n",
      "epoch: 17 train acc: 0.98536366 val acc: 0.9754\n",
      "epoch: 18 train acc: 0.98765457 val acc: 0.9766\n",
      "epoch: 19 train acc: 0.9881091 val acc: 0.9768\n",
      "epoch: 20 train acc: 0.98774546 val acc: 0.975\n",
      "epoch: 21 train acc: 0.9887273 val acc: 0.9774\n",
      "epoch: 22 train acc: 0.9900182 val acc: 0.975\n",
      "epoch: 23 train acc: 0.98934543 val acc: 0.9766\n",
      "epoch: 24 train acc: 0.9905818 val acc: 0.9756\n",
      "epoch: 25 train acc: 0.9902909 val acc: 0.9756\n",
      "epoch: 26 train acc: 0.9893636 val acc: 0.978\n",
      "epoch: 27 train acc: 0.9908364 val acc: 0.9758\n",
      "epoch: 28 train acc: 0.99114543 val acc: 0.9774\n",
      "epoch: 29 train acc: 0.99183637 val acc: 0.979\n",
      "[Test Accuracy] : 0.9766\n"
     ]
    }
   ],
   "source": [
    "# initialize\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# train hyperparameters\n",
    "epoch_cnt = 30\n",
    "batch_size = 1000\n",
    "iteration = len(X_train) // batch_size\n",
    "\n",
    "# train\n",
    "with tf.Session() as sess:\n",
    "    # run the initializer\n",
    "    sess.run(init)\n",
    "    pre_train_acc = 0.0\n",
    "    max_val_acc = 0.0\n",
    "    \n",
    "    for epoch in range(epoch_cnt):\n",
    "        avg_loss = 0.\n",
    "        start = 0\n",
    "        end = batch_size\n",
    "        \n",
    "        for i in range(iteration):\n",
    "            _, loss = sess.run([optimizer, cost],\n",
    "                              feed_dict={X:X_train[start:end], Y:y_train[start:end], keep_prob : 0.7})\n",
    "            start += batch_size\n",
    "            end += batch_size\n",
    "            \n",
    "            # compute average loss\n",
    "            avg_loss += loss/iteration\n",
    "        \n",
    "        # validate model\n",
    "        preds = tf.nn.softmax(hypothesis) # apply softmax to logits\n",
    "        correct_prediction = tf.equal(tf.argmax(preds, 1), tf.argmax(Y, 1))\n",
    "        \n",
    "        # calculate accuracy\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "        cur_train_acc = accuracy.eval({X:X_train, Y:y_train, keep_prob:1.0})\n",
    "        cur_val_acc = accuracy.eval({X:X_val, Y:y_val, keep_prob:1.0})\n",
    "        \n",
    "        print(\"epoch:\", epoch, \"train acc:\", cur_train_acc, \"val acc:\", cur_val_acc)\n",
    "        \n",
    "    # test model\n",
    "    preds = tf.nn.softmax(hypothesis)\n",
    "    correct_prediction = tf.equal(tf.argmax(preds, 1), tf.argmax(Y, 1))\n",
    "    \n",
    "    # calculate accuracy\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "    print(\"[Test Accuracy] :\", accuracy.eval({X:X_test, Y:y_test, keep_prob:1}))   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. 4-layer, relu, AdamOpimizer, Xavier, dropout, Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "Y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "# hidden layer 1\n",
    "W1_ = tf.get_variable(\"W1_\", shape=[784, 256], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b1_ = tf.get_variable(\"b1_\", [256], initializer=tf.zeros_initializer())\n",
    "L1 = tf.nn.relu(tf.matmul(X, W1_) + b1_)\n",
    "L1 = tf.nn.dropout(L1, keep_prob=keep_prob)\n",
    "\n",
    "# hidden layer 2\n",
    "W2_ = tf.get_variable(\"W2_\", shape=[256, 256], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b2_ = tf.get_variable(\"b2_\", [256], initializer=tf.zeros_initializer())\n",
    "L2 = tf.nn.relu(tf.matmul(L1, W2_) + b2_)\n",
    "L2 = tf.nn.dropout(L2, keep_prob=keep_prob)\n",
    "\n",
    "# hidden layer 3\n",
    "W3_ = tf.get_variable(\"W3_\", shape=[256, 256], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b3_ = tf.get_variable(\"b3_\", [256], initializer=tf.zeros_initializer())\n",
    "L3 = tf.nn.relu(tf.matmul(L2, W3_) + b3_)\n",
    "L3 = tf.nn.dropout(L3, keep_prob=keep_prob)\n",
    "\n",
    "# hidden layer 4\n",
    "W4_ = tf.get_variable(\"W4_\", shape=[256, 256], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b4_ = tf.get_variable(\"b4_\", [256], initializer=tf.zeros_initializer())\n",
    "L4 = tf.nn.relu(tf.matmul(L3, W4_) + b4_)\n",
    "L4 = tf.nn.dropout(L4, keep_prob=keep_prob)\n",
    "\n",
    "# output layer\n",
    "W5_ = tf.get_variable(\"W5_\", shape=[256, 10], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b5_ = tf.get_variable(\"b5_\", [10], initializer=tf.zeros_initializer())\n",
    "hypothesis = tf.matmul(L4, W5_) + b5_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits = hypothesis, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 train acc: 0.91965455 val acc: 0.9256\n",
      "epoch: 1 train acc: 0.95347273 val acc: 0.9556\n",
      "epoch: 2 train acc: 0.96450907 val acc: 0.963\n",
      "epoch: 3 train acc: 0.97034544 val acc: 0.9696\n",
      "epoch: 4 train acc: 0.9775091 val acc: 0.9722\n",
      "epoch: 5 train acc: 0.9802 val acc: 0.9738\n",
      "epoch: 6 train acc: 0.9852909 val acc: 0.9766\n",
      "epoch: 7 train acc: 0.98603636 val acc: 0.9752\n",
      "overfitting warning 0\n",
      "epoch: 8 train acc: 0.9890182 val acc: 0.9776\n",
      "epoch: 9 train acc: 0.99036366 val acc: 0.978\n",
      "epoch: 10 train acc: 0.9913273 val acc: 0.9796\n",
      "epoch: 11 train acc: 0.9920545 val acc: 0.9796\n",
      "epoch: 12 train acc: 0.9924909 val acc: 0.9782\n",
      "overfitting warning 0\n",
      "epoch: 13 train acc: 0.9938 val acc: 0.9796\n",
      "epoch: 14 train acc: 0.99398184 val acc: 0.9808\n",
      "epoch: 15 train acc: 0.9954 val acc: 0.9812\n",
      "epoch: 16 train acc: 0.99523634 val acc: 0.9802\n",
      "overfitting warning 0\n",
      "epoch: 17 train acc: 0.9960182 val acc: 0.98\n",
      "overfitting warning 1\n",
      "epoch: 18 train acc: 0.9959818 val acc: 0.9796\n",
      "overfitting warning 2\n",
      "epoch: 19 train acc: 0.9974545 val acc: 0.9812\n",
      "epoch: 20 train acc: 0.9968727 val acc: 0.9814\n",
      "epoch: 21 train acc: 0.99709094 val acc: 0.9816\n",
      "epoch: 22 train acc: 0.99618185 val acc: 0.9826\n",
      "epoch: 23 train acc: 0.9978 val acc: 0.9822\n",
      "overfitting warning 0\n",
      "epoch: 24 train acc: 0.9976909 val acc: 0.981\n",
      "overfitting warning 1\n",
      "epoch: 25 train acc: 0.9978909 val acc: 0.9824\n",
      "overfitting warning 2\n",
      "epoch: 26 train acc: 0.99774545 val acc: 0.981\n",
      "overfitting warning 3\n",
      "epoch: 27 train acc: 0.9978 val acc: 0.9818\n",
      "overfitting warning 4\n",
      "epoch: 28 train acc: 0.9981091 val acc: 0.9816\n",
      "early stopped on 28\n"
     ]
    }
   ],
   "source": [
    "# initialize\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# train hyperparameters\n",
    "epoch_cnt = 30\n",
    "batch_size = 1000\n",
    "iteration = len(X_train) // batch_size\n",
    "saver = tf.train.Saver()\n",
    "earlystop_threshold = 5\n",
    "earlystop_cnt = 0\n",
    "\n",
    "# train\n",
    "with tf.Session() as sess:\n",
    "    # run the initializer\n",
    "    sess.run(init)\n",
    "    pre_train_acc = 0.0\n",
    "    max_val_acc = 0.0\n",
    "    \n",
    "    for epoch in range(epoch_cnt):\n",
    "        avg_loss = 0.\n",
    "        start = 0\n",
    "        end = batch_size\n",
    "        \n",
    "        for i in range(iteration):\n",
    "            _, loss = sess.run([optimizer, cost],\n",
    "                              feed_dict={X:X_train[start:end], Y:y_train[start:end], keep_prob : 0.7})\n",
    "            start += batch_size\n",
    "            end += batch_size\n",
    "            \n",
    "            # compute average loss\n",
    "            avg_loss += loss/iteration\n",
    "        \n",
    "        # validate model\n",
    "        preds = tf.nn.softmax(hypothesis) # apply softmax to logits\n",
    "        correct_prediction = tf.equal(tf.argmax(preds, 1), tf.argmax(Y, 1))\n",
    "        \n",
    "        # calculate accuracy\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "        cur_train_acc = accuracy.eval({X:X_train, Y:y_train, keep_prob:1.0})\n",
    "        cur_val_acc = accuracy.eval({X:X_val, Y:y_val, keep_prob:1.0})\n",
    "        \n",
    "        print(\"epoch:\", epoch, \"train acc:\", cur_train_acc, \"val acc:\", cur_val_acc)\n",
    "        \n",
    "        if cur_val_acc < max_val_acc:\n",
    "            if cur_train_acc > prev_train_acc or cur_train_acc > 0.99:\n",
    "                if earlystop_cnt == earlystop_threshold:\n",
    "                    print(\"early stopped on\", epoch)\n",
    "                    break\n",
    "                else:\n",
    "                    print(\"overfitting warning\", earlystop_cnt)\n",
    "                earlystop_cnt += 1\n",
    "            else:\n",
    "                earlystop_cnt = 0\n",
    "        else:\n",
    "            earlystop_cnt = 0\n",
    "            max_val_acc = cur_val_acc\n",
    "            \n",
    "        # save the variables to file\n",
    "        save_path = saver.save(sess, \"model.ckpt\")\n",
    "        prev_train_acc = cur_train_acc\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from model.ckpt\n",
      "[Test Accuracy] : 0.9824\n"
     ]
    }
   ],
   "source": [
    "# test model\n",
    "with tf.Session() as sess:\n",
    "    # restore variables\n",
    "    saver.restore(sess, \"model.ckpt\")\n",
    "    correct_prediction = tf.equal(tf.argmax(preds, 1), tf.argmax(Y, 1))\n",
    "    \n",
    "    # calculate accuracy\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "    print(\"[Test Accuracy] :\", accuracy.eval({X:X_test, Y:y_test, keep_prob:1.0}))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
