{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Activation,Flatten, Conv2D, MaxPooling2D\n",
    "from keras.layers import Input, ZeroPadding2D, BatchNormalization, Add, AveragePooling2D\n",
    "from keras import backend\n",
    "from keras.initializers import glorot_uniform\n",
    "from keras.preprocessing import image\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "backend.set_image_data_format('channels_last')\n",
    "train_path = './Dataset/train'\n",
    "test_path = './Dataset/test'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the augmentation tool for data augmentation\n",
    "image_gen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=15,\n",
    "    width_shift_range=.15,\n",
    "    height_shift_range=.15,\n",
    "    horizontal_flip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augmentation(generator):      \n",
    "    \"\"\"\n",
    "    :param tuple generator: ImageDataGenerator\n",
    "    :return array: transformed tenfold images\n",
    "    \"\"\"\n",
    "    \n",
    "    classes_list = os.listdir(train_path)\n",
    "\n",
    "    for class_name in classes_list:\n",
    "        img_list = os.listdir(os.path.join(train_path, class_name))\n",
    "\n",
    "    for img_name in img_list:\n",
    "            img_path_name = os.path.join(train_path, class_name, img_name)\n",
    "            img = image.load_img(img_path_name)\n",
    "            img = image.img_to_array(img)\n",
    "\n",
    "            img = img.reshape((1,) + img.shape)\n",
    "\n",
    "            i = 0\n",
    "\n",
    "            for batch in generator.flow(img, batch_size=1, save_to_dir=os.path.join(train_path, class_name),\n",
    "                save_prefix=class_name, save_format='jpg'):\n",
    "                i += 1\n",
    "                if i > 10:\n",
    "                    break\n",
    "\n",
    "augmentation(image_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_preprocessed_data(path):\n",
    "    \"\"\"\n",
    "    :param string path: train_path or test_path\n",
    "    :return array, array: dataset(features), Y(label)\n",
    "    \"\"\"\n",
    "    \n",
    "    dataset = []\n",
    "    y = []\n",
    "\n",
    "    classes_list = os.listdir(path)\n",
    "\n",
    "    for class_number, class_name in enumerate(classes_list):\n",
    "        img_list = os.listdir(os.path.join(path, class_name))\n",
    "\n",
    "        for img_number, img_name in enumerate(img_list):\n",
    "            img_path_name = os.path.join(path, class_name, img_name)\n",
    "\n",
    "            img = image.load_img(img_path_name, target_size=(64, 64))\n",
    "\n",
    "            # img -> np.array\n",
    "            # shape : (height, width, channels) = (128, 128, 3)\n",
    "            img_input = image.img_to_array(img)\n",
    "            dataset.append(img_input)\n",
    "            y.append(class_number)\n",
    "    \n",
    "    dataset = np.array(dataset)\n",
    "    y = np.array(y)\n",
    "    Y = np.eye(3)[y.astype(int)]\n",
    "    dataset = dataset.astype('float64')\n",
    "    dataset /= 255\n",
    "    \n",
    "    return dataset, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling - ResNet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identity block\n",
    "\n",
    "def identity_block(X, kernel, filters, stage, block):\n",
    "    \"\"\"\n",
    "    :param tensor X: input tensor (examples, height_prev, width_prev, channel_prev)\n",
    "    :param int kernel: kernel size\n",
    "    :param list filters: filsters of CONV layer\n",
    "    :param int stage: n-th stage\n",
    "    :param string block: m-th block in nth stage\n",
    "\n",
    "    :return tensor: output of the identity block - tensor: (height, width, channel)\n",
    "    \"\"\"\n",
    "    \n",
    "    # setting\n",
    "    conv_name_base = 'res' + str(stage) + block + '_branch'\n",
    "    bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
    "    F1, F2, F3 = filters\n",
    "    X_shortcut = X\n",
    "\n",
    "    # 1st component\n",
    "    X = Conv2D(filters=F1, kernel_size=(1, 1), strides=(1, 1), padding='valid', name=conv_name_base+'2a',\n",
    "               kernel_initializer=glorot_uniform(seed=0))(X)\n",
    "    X = BatchNormalization(axis=3, name=bn_name_base + '2a')(X)\n",
    "    X = Activation('relu')(X)\n",
    "\n",
    "    # 2nd compoenent\n",
    "    X = Conv2D(filters=F2, kernel_size=(kernel, kernel), strides=(1, 1), padding='same', name=conv_name_base+'2b',\n",
    "               kernel_initializer=glorot_uniform(seed=0))(X)\n",
    "    X = BatchNormalization(axis=3, name=bn_name_base + '2b')(X)\n",
    "    X = Activation('relu')(X)\n",
    "\n",
    "    # 3rd component\n",
    "    X = Conv2D(filters=F3, kernel_size=(1, 1), strides=(1, 1), padding='valid', name=conv_name_base+'2c',\n",
    "               kernel_initializer=glorot_uniform(seed=0))(X)\n",
    "    X = BatchNormalization(axis=3, name=bn_name_base + '2c')(X)\n",
    "\n",
    "    # add shorcut again\n",
    "    X = Add()([X, X_shortcut])\n",
    "    X = Activation('relu')(X)\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_block(X, kernel, filters, stage, block, s=2):    \n",
    "    \"\"\"\n",
    "    :param tensor X: input tensor (examples, height_prev, width_prev, channel_prev)\n",
    "    :param int kernel: kernel size\n",
    "    :param list filters: filsters of CONV layer\n",
    "    :param int stage: n-th stage\n",
    "    :param string block: m-th block in nth stage\n",
    "    :param int s: strides\n",
    "\n",
    "    :return tensor: X -- output of the convolutional block, tensor: (height, width, channel)\n",
    "    \"\"\"\n",
    "    \n",
    "    # setting\n",
    "    conv_name_base = 'res' + str(stage) + block + '_branch'\n",
    "    bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
    "    F1, F2, F3 = filters\n",
    "    X_shortcut = X\n",
    "\n",
    "    # 1st component\n",
    "    X = Conv2D(filters=F1, kernel_size=(1, 1), strides=(s, s), name=conv_name_base+'2a', kernel_initializer=glorot_uniform(seed=0))(X)\n",
    "    X = BatchNormalization(axis=3, name=bn_name_base+'2a')(X)\n",
    "    X = Activation('relu')(X)\n",
    "\n",
    "    # 2nd compoenent\n",
    "    X = Conv2D(filters=F2, kernel_size=(kernel, kernel), strides=(1, 1), name=conv_name_base+'2b', padding='same', kernel_initializer=glorot_uniform(seed=0))(X)\n",
    "    X = BatchNormalization(axis=3, name=bn_name_base+'2b')(X)\n",
    "    X = Activation('relu')(X)\n",
    "\n",
    "    # 3rd component\n",
    "    X = Conv2D(filters=F3, kernel_size=(1, 1), strides=(1, 1), name=conv_name_base+'2c', kernel_initializer=glorot_uniform(seed=0))(X)\n",
    "    X = BatchNormalization(axis=3, name=bn_name_base+'2c')(X)\n",
    "\n",
    "    # apply 1X1 conv to shortcut as well\n",
    "    X_shortcut = Conv2D(filters=F3, kernel_size=(1, 1), strides=(s, s), name=conv_name_base+'1', kernel_initializer=glorot_uniform(seed=0))(X_shortcut)\n",
    "    X_shortcut = BatchNormalization(axis=3, name=bn_name_base+'1')(X_shortcut)\n",
    "\n",
    "    # add shorcut again\n",
    "    X = Add()([X, X_shortcut])\n",
    "    X = Activation('relu')(X)\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(Model):\n",
    "    def __init__(self, in_shape=(128, 128, 3), classes=3):\n",
    "        self.in_shape = in_shape\n",
    "        self.classes = classes\n",
    "        self.build_model()\n",
    "        super().__init__(self.X_input, self.Y)\n",
    "        self.compile()\n",
    "\n",
    "\n",
    "    def build_model(self):\n",
    "        in_shape = self.in_shape\n",
    "        classes = self.classes\n",
    "        X_input = Input(shape=in_shape)\n",
    "        X = ZeroPadding2D((3, 3))(X_input)\n",
    "\n",
    "        # Stage 1\n",
    "        X = Conv2D(filters=64, kernel_size=(7, 7), strides=(2, 2), name='conv1',\n",
    "                   kernel_initializer=glorot_uniform(seed=0))(X)\n",
    "        X = BatchNormalization(axis=3, name='bn_conv1')(X)\n",
    "        X = Activation('relu')(X)\n",
    "        X = MaxPooling2D((3, 3), strides=(2, 2))(X)\n",
    "\n",
    "        # Stage 2\n",
    "        X = conv_block(X, kernel=3, filters=[64, 64, 256], stage=2, block='a', s=1)\n",
    "        X = identity_block(X, kernel=3, filters=[64, 64, 256], stage=2, block='b')\n",
    "        X = identity_block(X, kernel=3, filters=[64, 64, 256], stage=2, block='c')\n",
    "\n",
    "        # Stage 3\n",
    "        X = conv_block(X, kernel=3, filters=[128, 128, 512], stage=3, block='a', s=2)\n",
    "        X = identity_block(X, kernel=3, filters=[128, 128, 512], stage=3, block='b')\n",
    "        X = identity_block(X, kernel=3, filters=[128, 128, 512], stage=3, block='c')\n",
    "        X = identity_block(X, kernel=3, filters=[128, 128, 512], stage=3, block='d')\n",
    "\n",
    "        # Stage 4\n",
    "        X = conv_block(X, kernel=3, filters=[256, 256, 1024], stage=4, block='a', s=2)\n",
    "        X = identity_block(X, kernel=3, filters=[256, 256, 1024], stage=4, block='b')\n",
    "        X = identity_block(X, kernel=3, filters=[256, 256, 1024], stage=4, block='c')\n",
    "        X = identity_block(X, kernel=3, filters=[256, 256, 1024], stage=4, block='d')\n",
    "        X = identity_block(X, kernel=3, filters=[256, 256, 1024], stage=4, block='e')\n",
    "        X = identity_block(X, kernel=3, filters=[256, 256, 1024], stage=4, block='kernel')\n",
    "\n",
    "        # Stage 5\n",
    "        X = conv_block(X, kernel=3, filters=[512, 512, 2048], stage=5, block='a', s=2)\n",
    "        X = identity_block(X, kernel=3, filters=[512, 512, 2048], stage=5, block='b')\n",
    "        X = identity_block(X, kernel=3, filters=[512, 512, 2048], stage=5, block='c')\n",
    "\n",
    "        X = AveragePooling2D(pool_size=(2, 2), strides=None, padding='valid', name='avg_pool')(X)\n",
    "        X = Flatten()(X)\n",
    "        Y = Dense(units=classes, activation='softmax', name='fc' + str(classes), kernel_initializer=glorot_uniform(seed=0))(X)\n",
    "\n",
    "        self.X_input, self.Y = X_input, Y\n",
    "\n",
    "    def compile(self):\n",
    "        Model.compile(self, optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[ 53.  47.  47.]\n",
      "   [ 38.  39.  39.]\n",
      "   [ 36.  39.  42.]\n",
      "   ...\n",
      "   [ 64.  71.  79.]\n",
      "   [ 63.  70.  78.]\n",
      "   [ 65.  72.  80.]]\n",
      "\n",
      "  [[ 51.  46.  48.]\n",
      "   [ 40.  42.  43.]\n",
      "   [ 37.  42.  45.]\n",
      "   ...\n",
      "   [ 64.  71.  79.]\n",
      "   [ 62.  69.  77.]\n",
      "   [ 66.  73.  81.]]\n",
      "\n",
      "  [[ 48.  45.  49.]\n",
      "   [ 41.  46.  49.]\n",
      "   [ 39.  46.  50.]\n",
      "   ...\n",
      "   [ 63.  70.  78.]\n",
      "   [ 64.  71.  79.]\n",
      "   [ 69.  76.  84.]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[173. 127. 112.]\n",
      "   [173. 126. 111.]\n",
      "   [171. 124. 109.]\n",
      "   ...\n",
      "   [ 40.  43.  48.]\n",
      "   [ 41.  44.  49.]\n",
      "   [ 41.  44.  49.]]\n",
      "\n",
      "  [[162. 118. 105.]\n",
      "   [162. 118. 105.]\n",
      "   [161. 117. 104.]\n",
      "   ...\n",
      "   [ 43.  46.  51.]\n",
      "   [ 41.  44.  49.]\n",
      "   [ 43.  46.  51.]]\n",
      "\n",
      "  [[153. 111.  99.]\n",
      "   [154. 112. 100.]\n",
      "   [156. 114. 102.]\n",
      "   ...\n",
      "   [ 43.  46.  51.]\n",
      "   [ 42.  45.  50.]\n",
      "   [ 43.  46.  51.]]]\n",
      "\n",
      "\n",
      " [[[ 46.  47.  52.]\n",
      "   [ 48.  51.  56.]\n",
      "   [ 48.  52.  56.]\n",
      "   ...\n",
      "   [ 61.  69.  72.]\n",
      "   [ 58.  66.  69.]\n",
      "   [ 62.  70.  73.]]\n",
      "\n",
      "  [[ 51.  52.  57.]\n",
      "   [ 51.  54.  59.]\n",
      "   [ 51.  56.  60.]\n",
      "   ...\n",
      "   [ 61.  69.  72.]\n",
      "   [ 63.  71.  74.]\n",
      "   [ 61.  69.  72.]]\n",
      "\n",
      "  [[ 53.  54.  59.]\n",
      "   [ 53.  55.  60.]\n",
      "   [ 51.  55.  60.]\n",
      "   ...\n",
      "   [ 63.  72.  74.]\n",
      "   [ 63.  71.  74.]\n",
      "   [ 61.  69.  72.]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[183. 134. 120.]\n",
      "   [181. 132. 118.]\n",
      "   [181. 133. 119.]\n",
      "   ...\n",
      "   [ 43.  46.  51.]\n",
      "   [ 44.  47.  52.]\n",
      "   [ 42.  45.  50.]]\n",
      "\n",
      "  [[174. 127. 115.]\n",
      "   [171. 125. 112.]\n",
      "   [171. 124. 111.]\n",
      "   ...\n",
      "   [ 43.  46.  51.]\n",
      "   [ 42.  45.  50.]\n",
      "   [ 41.  44.  49.]]\n",
      "\n",
      "  [[166. 122. 111.]\n",
      "   [159. 114. 103.]\n",
      "   [165. 120. 109.]\n",
      "   ...\n",
      "   [ 42.  45.  50.]\n",
      "   [ 41.  44.  49.]\n",
      "   [ 44.  47.  52.]]]\n",
      "\n",
      "\n",
      " [[[ 42.  43.  48.]\n",
      "   [ 42.  43.  48.]\n",
      "   [ 46.  47.  52.]\n",
      "   ...\n",
      "   [ 48.  50.  62.]\n",
      "   [ 47.  49.  61.]\n",
      "   [ 48.  50.  62.]]\n",
      "\n",
      "  [[ 42.  43.  48.]\n",
      "   [ 42.  43.  48.]\n",
      "   [ 43.  44.  49.]\n",
      "   ...\n",
      "   [ 45.  47.  57.]\n",
      "   [ 47.  50.  60.]\n",
      "   [ 49.  51.  61.]]\n",
      "\n",
      "  [[ 39.  40.  45.]\n",
      "   [ 40.  41.  46.]\n",
      "   [ 42.  43.  48.]\n",
      "   ...\n",
      "   [ 44.  47.  54.]\n",
      "   [ 47.  50.  57.]\n",
      "   [ 49.  52.  59.]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[ 22.  23.  27.]\n",
      "   [ 22.  23.  27.]\n",
      "   [ 20.  21.  25.]\n",
      "   ...\n",
      "   [ 38.  41.  46.]\n",
      "   [ 37.  40.  45.]\n",
      "   [ 41.  44.  49.]]\n",
      "\n",
      "  [[ 24.  25.  29.]\n",
      "   [ 24.  25.  29.]\n",
      "   [ 23.  24.  28.]\n",
      "   ...\n",
      "   [ 37.  40.  45.]\n",
      "   [ 38.  41.  46.]\n",
      "   [ 38.  41.  46.]]\n",
      "\n",
      "  [[ 24.  25.  29.]\n",
      "   [ 24.  25.  29.]\n",
      "   [ 24.  25.  29.]\n",
      "   ...\n",
      "   [ 39.  42.  47.]\n",
      "   [ 40.  43.  48.]\n",
      "   [ 40.  43.  48.]]]\n",
      "\n",
      "\n",
      " ...\n",
      "\n",
      "\n",
      " [[[  8.  11.  16.]\n",
      "   [ 10.  13.  18.]\n",
      "   [ 11.  14.  19.]\n",
      "   ...\n",
      "   [ 24.  27.  32.]\n",
      "   [ 25.  28.  33.]\n",
      "   [ 25.  28.  33.]]\n",
      "\n",
      "  [[  8.  11.  16.]\n",
      "   [ 10.  13.  18.]\n",
      "   [ 13.  16.  21.]\n",
      "   ...\n",
      "   [ 23.  26.  31.]\n",
      "   [ 23.  26.  31.]\n",
      "   [ 23.  26.  31.]]\n",
      "\n",
      "  [[  8.  11.  16.]\n",
      "   [  8.  11.  16.]\n",
      "   [ 14.  17.  22.]\n",
      "   ...\n",
      "   [ 22.  25.  30.]\n",
      "   [ 22.  25.  30.]\n",
      "   [ 23.  26.  31.]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[ 19.  23.  24.]\n",
      "   [ 17.  21.  22.]\n",
      "   [ 18.  22.  23.]\n",
      "   ...\n",
      "   [ 17.  18.  22.]\n",
      "   [ 21.  22.  26.]\n",
      "   [ 21.  22.  26.]]\n",
      "\n",
      "  [[ 15.  19.  20.]\n",
      "   [ 14.  18.  19.]\n",
      "   [ 14.  18.  19.]\n",
      "   ...\n",
      "   [ 17.  18.  22.]\n",
      "   [ 21.  22.  26.]\n",
      "   [ 20.  21.  25.]]\n",
      "\n",
      "  [[ 15.  19.  20.]\n",
      "   [ 14.  18.  19.]\n",
      "   [ 15.  19.  20.]\n",
      "   ...\n",
      "   [ 16.  17.  21.]\n",
      "   [ 20.  21.  25.]\n",
      "   [ 19.  20.  24.]]]\n",
      "\n",
      "\n",
      " [[[ 58.  67.  77.]\n",
      "   [ 56.  65.  76.]\n",
      "   [ 54.  64.  74.]\n",
      "   ...\n",
      "   [ 24.  26.  38.]\n",
      "   [ 30.  32.  44.]\n",
      "   [ 27.  29.  41.]]\n",
      "\n",
      "  [[ 58.  66.  77.]\n",
      "   [ 55.  63.  74.]\n",
      "   [ 52.  60.  71.]\n",
      "   ...\n",
      "   [ 34.  36.  48.]\n",
      "   [ 33.  35.  47.]\n",
      "   [ 34.  36.  48.]]\n",
      "\n",
      "  [[ 61.  66.  78.]\n",
      "   [ 58.  64.  76.]\n",
      "   [ 56.  61.  73.]\n",
      "   ...\n",
      "   [ 37.  39.  51.]\n",
      "   [ 36.  38.  50.]\n",
      "   [ 34.  36.  48.]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[ 58.  46.  49.]\n",
      "   [ 33.  24.  28.]\n",
      "   [ 21.  16.  23.]\n",
      "   ...\n",
      "   [ 18.  20.  32.]\n",
      "   [ 19.  20.  33.]\n",
      "   [ 18.  20.  32.]]\n",
      "\n",
      "  [[ 16.  17.  27.]\n",
      "   [ 15.  17.  27.]\n",
      "   [ 15.  17.  28.]\n",
      "   ...\n",
      "   [ 19.  21.  33.]\n",
      "   [ 18.  20.  32.]\n",
      "   [ 19.  21.  33.]]\n",
      "\n",
      "  [[ 16.  15.  28.]\n",
      "   [ 16.  15.  27.]\n",
      "   [ 18.  14.  25.]\n",
      "   ...\n",
      "   [ 19.  21.  33.]\n",
      "   [ 17.  19.  31.]\n",
      "   [ 17.  19.  31.]]]\n",
      "\n",
      "\n",
      " [[[ 49.  62.  71.]\n",
      "   [ 50.  63.  72.]\n",
      "   [ 50.  63.  72.]\n",
      "   ...\n",
      "   [ 46.  53.  61.]\n",
      "   [ 50.  57.  65.]\n",
      "   [ 48.  55.  63.]]\n",
      "\n",
      "  [[ 44.  57.  66.]\n",
      "   [ 45.  58.  67.]\n",
      "   [ 45.  58.  67.]\n",
      "   ...\n",
      "   [ 48.  55.  63.]\n",
      "   [ 49.  56.  64.]\n",
      "   [ 48.  55.  63.]]\n",
      "\n",
      "  [[ 40.  53.  62.]\n",
      "   [ 40.  53.  62.]\n",
      "   [ 42.  55.  64.]\n",
      "   ...\n",
      "   [ 49.  56.  64.]\n",
      "   [ 48.  55.  63.]\n",
      "   [ 48.  55.  63.]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[ 46.  43.  41.]\n",
      "   [ 48.  44.  42.]\n",
      "   [ 54.  46.  46.]\n",
      "   ...\n",
      "   [ 43.  48.  52.]\n",
      "   [ 43.  48.  52.]\n",
      "   [ 43.  48.  52.]]\n",
      "\n",
      "  [[ 34.  39.  42.]\n",
      "   [ 34.  37.  40.]\n",
      "   [ 32.  33.  37.]\n",
      "   ...\n",
      "   [ 45.  50.  54.]\n",
      "   [ 46.  51.  55.]\n",
      "   [ 45.  50.  54.]]\n",
      "\n",
      "  [[ 30.  37.  39.]\n",
      "   [ 30.  35.  38.]\n",
      "   [ 28.  30.  34.]\n",
      "   ...\n",
      "   [ 51.  56.  60.]\n",
      "   [ 49.  54.  58.]\n",
      "   [ 47.  52.  56.]]]]\n",
      "[[[[ 53.  47.  47.]\n",
      "   [ 38.  39.  39.]\n",
      "   [ 36.  39.  42.]\n",
      "   ...\n",
      "   [ 64.  71.  79.]\n",
      "   [ 63.  70.  78.]\n",
      "   [ 65.  72.  80.]]\n",
      "\n",
      "  [[ 51.  46.  48.]\n",
      "   [ 40.  42.  43.]\n",
      "   [ 37.  42.  45.]\n",
      "   ...\n",
      "   [ 64.  71.  79.]\n",
      "   [ 62.  69.  77.]\n",
      "   [ 66.  73.  81.]]\n",
      "\n",
      "  [[ 48.  45.  49.]\n",
      "   [ 41.  46.  49.]\n",
      "   [ 39.  46.  50.]\n",
      "   ...\n",
      "   [ 63.  70.  78.]\n",
      "   [ 64.  71.  79.]\n",
      "   [ 69.  76.  84.]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[173. 127. 112.]\n",
      "   [173. 126. 111.]\n",
      "   [171. 124. 109.]\n",
      "   ...\n",
      "   [ 40.  43.  48.]\n",
      "   [ 41.  44.  49.]\n",
      "   [ 41.  44.  49.]]\n",
      "\n",
      "  [[162. 118. 105.]\n",
      "   [162. 118. 105.]\n",
      "   [161. 117. 104.]\n",
      "   ...\n",
      "   [ 43.  46.  51.]\n",
      "   [ 41.  44.  49.]\n",
      "   [ 43.  46.  51.]]\n",
      "\n",
      "  [[153. 111.  99.]\n",
      "   [154. 112. 100.]\n",
      "   [156. 114. 102.]\n",
      "   ...\n",
      "   [ 43.  46.  51.]\n",
      "   [ 42.  45.  50.]\n",
      "   [ 43.  46.  51.]]]\n",
      "\n",
      "\n",
      " [[[ 46.  47.  52.]\n",
      "   [ 48.  51.  56.]\n",
      "   [ 48.  52.  56.]\n",
      "   ...\n",
      "   [ 61.  69.  72.]\n",
      "   [ 58.  66.  69.]\n",
      "   [ 62.  70.  73.]]\n",
      "\n",
      "  [[ 51.  52.  57.]\n",
      "   [ 51.  54.  59.]\n",
      "   [ 51.  56.  60.]\n",
      "   ...\n",
      "   [ 61.  69.  72.]\n",
      "   [ 63.  71.  74.]\n",
      "   [ 61.  69.  72.]]\n",
      "\n",
      "  [[ 53.  54.  59.]\n",
      "   [ 53.  55.  60.]\n",
      "   [ 51.  55.  60.]\n",
      "   ...\n",
      "   [ 63.  72.  74.]\n",
      "   [ 63.  71.  74.]\n",
      "   [ 61.  69.  72.]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[183. 134. 120.]\n",
      "   [181. 132. 118.]\n",
      "   [181. 133. 119.]\n",
      "   ...\n",
      "   [ 43.  46.  51.]\n",
      "   [ 44.  47.  52.]\n",
      "   [ 42.  45.  50.]]\n",
      "\n",
      "  [[174. 127. 115.]\n",
      "   [171. 125. 112.]\n",
      "   [171. 124. 111.]\n",
      "   ...\n",
      "   [ 43.  46.  51.]\n",
      "   [ 42.  45.  50.]\n",
      "   [ 41.  44.  49.]]\n",
      "\n",
      "  [[166. 122. 111.]\n",
      "   [159. 114. 103.]\n",
      "   [165. 120. 109.]\n",
      "   ...\n",
      "   [ 42.  45.  50.]\n",
      "   [ 41.  44.  49.]\n",
      "   [ 44.  47.  52.]]]\n",
      "\n",
      "\n",
      " [[[ 42.  43.  48.]\n",
      "   [ 42.  43.  48.]\n",
      "   [ 46.  47.  52.]\n",
      "   ...\n",
      "   [ 48.  50.  62.]\n",
      "   [ 47.  49.  61.]\n",
      "   [ 48.  50.  62.]]\n",
      "\n",
      "  [[ 42.  43.  48.]\n",
      "   [ 42.  43.  48.]\n",
      "   [ 43.  44.  49.]\n",
      "   ...\n",
      "   [ 45.  47.  57.]\n",
      "   [ 47.  50.  60.]\n",
      "   [ 49.  51.  61.]]\n",
      "\n",
      "  [[ 39.  40.  45.]\n",
      "   [ 40.  41.  46.]\n",
      "   [ 42.  43.  48.]\n",
      "   ...\n",
      "   [ 44.  47.  54.]\n",
      "   [ 47.  50.  57.]\n",
      "   [ 49.  52.  59.]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[ 22.  23.  27.]\n",
      "   [ 22.  23.  27.]\n",
      "   [ 20.  21.  25.]\n",
      "   ...\n",
      "   [ 38.  41.  46.]\n",
      "   [ 37.  40.  45.]\n",
      "   [ 41.  44.  49.]]\n",
      "\n",
      "  [[ 24.  25.  29.]\n",
      "   [ 24.  25.  29.]\n",
      "   [ 23.  24.  28.]\n",
      "   ...\n",
      "   [ 37.  40.  45.]\n",
      "   [ 38.  41.  46.]\n",
      "   [ 38.  41.  46.]]\n",
      "\n",
      "  [[ 24.  25.  29.]\n",
      "   [ 24.  25.  29.]\n",
      "   [ 24.  25.  29.]\n",
      "   ...\n",
      "   [ 39.  42.  47.]\n",
      "   [ 40.  43.  48.]\n",
      "   [ 40.  43.  48.]]]\n",
      "\n",
      "\n",
      " ...\n",
      "\n",
      "\n",
      " [[[  8.  11.  16.]\n",
      "   [ 10.  13.  18.]\n",
      "   [ 11.  14.  19.]\n",
      "   ...\n",
      "   [ 24.  27.  32.]\n",
      "   [ 25.  28.  33.]\n",
      "   [ 25.  28.  33.]]\n",
      "\n",
      "  [[  8.  11.  16.]\n",
      "   [ 10.  13.  18.]\n",
      "   [ 13.  16.  21.]\n",
      "   ...\n",
      "   [ 23.  26.  31.]\n",
      "   [ 23.  26.  31.]\n",
      "   [ 23.  26.  31.]]\n",
      "\n",
      "  [[  8.  11.  16.]\n",
      "   [  8.  11.  16.]\n",
      "   [ 14.  17.  22.]\n",
      "   ...\n",
      "   [ 22.  25.  30.]\n",
      "   [ 22.  25.  30.]\n",
      "   [ 23.  26.  31.]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[ 19.  23.  24.]\n",
      "   [ 17.  21.  22.]\n",
      "   [ 18.  22.  23.]\n",
      "   ...\n",
      "   [ 17.  18.  22.]\n",
      "   [ 21.  22.  26.]\n",
      "   [ 21.  22.  26.]]\n",
      "\n",
      "  [[ 15.  19.  20.]\n",
      "   [ 14.  18.  19.]\n",
      "   [ 14.  18.  19.]\n",
      "   ...\n",
      "   [ 17.  18.  22.]\n",
      "   [ 21.  22.  26.]\n",
      "   [ 20.  21.  25.]]\n",
      "\n",
      "  [[ 15.  19.  20.]\n",
      "   [ 14.  18.  19.]\n",
      "   [ 15.  19.  20.]\n",
      "   ...\n",
      "   [ 16.  17.  21.]\n",
      "   [ 20.  21.  25.]\n",
      "   [ 19.  20.  24.]]]\n",
      "\n",
      "\n",
      " [[[ 58.  67.  77.]\n",
      "   [ 56.  65.  76.]\n",
      "   [ 54.  64.  74.]\n",
      "   ...\n",
      "   [ 24.  26.  38.]\n",
      "   [ 30.  32.  44.]\n",
      "   [ 27.  29.  41.]]\n",
      "\n",
      "  [[ 58.  66.  77.]\n",
      "   [ 55.  63.  74.]\n",
      "   [ 52.  60.  71.]\n",
      "   ...\n",
      "   [ 34.  36.  48.]\n",
      "   [ 33.  35.  47.]\n",
      "   [ 34.  36.  48.]]\n",
      "\n",
      "  [[ 61.  66.  78.]\n",
      "   [ 58.  64.  76.]\n",
      "   [ 56.  61.  73.]\n",
      "   ...\n",
      "   [ 37.  39.  51.]\n",
      "   [ 36.  38.  50.]\n",
      "   [ 34.  36.  48.]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[ 58.  46.  49.]\n",
      "   [ 33.  24.  28.]\n",
      "   [ 21.  16.  23.]\n",
      "   ...\n",
      "   [ 18.  20.  32.]\n",
      "   [ 19.  20.  33.]\n",
      "   [ 18.  20.  32.]]\n",
      "\n",
      "  [[ 16.  17.  27.]\n",
      "   [ 15.  17.  27.]\n",
      "   [ 15.  17.  28.]\n",
      "   ...\n",
      "   [ 19.  21.  33.]\n",
      "   [ 18.  20.  32.]\n",
      "   [ 19.  21.  33.]]\n",
      "\n",
      "  [[ 16.  15.  28.]\n",
      "   [ 16.  15.  27.]\n",
      "   [ 18.  14.  25.]\n",
      "   ...\n",
      "   [ 19.  21.  33.]\n",
      "   [ 17.  19.  31.]\n",
      "   [ 17.  19.  31.]]]\n",
      "\n",
      "\n",
      " [[[ 49.  62.  71.]\n",
      "   [ 50.  63.  72.]\n",
      "   [ 50.  63.  72.]\n",
      "   ...\n",
      "   [ 46.  53.  61.]\n",
      "   [ 50.  57.  65.]\n",
      "   [ 48.  55.  63.]]\n",
      "\n",
      "  [[ 44.  57.  66.]\n",
      "   [ 45.  58.  67.]\n",
      "   [ 45.  58.  67.]\n",
      "   ...\n",
      "   [ 48.  55.  63.]\n",
      "   [ 49.  56.  64.]\n",
      "   [ 48.  55.  63.]]\n",
      "\n",
      "  [[ 40.  53.  62.]\n",
      "   [ 40.  53.  62.]\n",
      "   [ 42.  55.  64.]\n",
      "   ...\n",
      "   [ 49.  56.  64.]\n",
      "   [ 48.  55.  63.]\n",
      "   [ 48.  55.  63.]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[ 46.  43.  41.]\n",
      "   [ 48.  44.  42.]\n",
      "   [ 54.  46.  46.]\n",
      "   ...\n",
      "   [ 43.  48.  52.]\n",
      "   [ 43.  48.  52.]\n",
      "   [ 43.  48.  52.]]\n",
      "\n",
      "  [[ 34.  39.  42.]\n",
      "   [ 34.  37.  40.]\n",
      "   [ 32.  33.  37.]\n",
      "   ...\n",
      "   [ 45.  50.  54.]\n",
      "   [ 46.  51.  55.]\n",
      "   [ 45.  50.  54.]]\n",
      "\n",
      "  [[ 30.  37.  39.]\n",
      "   [ 30.  35.  38.]\n",
      "   [ 28.  30.  34.]\n",
      "   ...\n",
      "   [ 51.  56.  60.]\n",
      "   [ 49.  54.  58.]\n",
      "   [ 47.  52.  56.]]]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[0.20784314 0.18431373 0.18431373]\n",
      "   [0.14901961 0.15294118 0.15294118]\n",
      "   [0.14117647 0.15294118 0.16470588]\n",
      "   ...\n",
      "   [0.25098039 0.27843137 0.30980392]\n",
      "   [0.24705882 0.2745098  0.30588235]\n",
      "   [0.25490196 0.28235294 0.31372549]]\n",
      "\n",
      "  [[0.2        0.18039216 0.18823529]\n",
      "   [0.15686275 0.16470588 0.16862745]\n",
      "   [0.14509804 0.16470588 0.17647059]\n",
      "   ...\n",
      "   [0.25098039 0.27843137 0.30980392]\n",
      "   [0.24313725 0.27058824 0.30196078]\n",
      "   [0.25882353 0.28627451 0.31764706]]\n",
      "\n",
      "  [[0.18823529 0.17647059 0.19215686]\n",
      "   [0.16078431 0.18039216 0.19215686]\n",
      "   [0.15294118 0.18039216 0.19607843]\n",
      "   ...\n",
      "   [0.24705882 0.2745098  0.30588235]\n",
      "   [0.25098039 0.27843137 0.30980392]\n",
      "   [0.27058824 0.29803922 0.32941176]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.67843137 0.49803922 0.43921569]\n",
      "   [0.67843137 0.49411765 0.43529412]\n",
      "   [0.67058824 0.48627451 0.42745098]\n",
      "   ...\n",
      "   [0.15686275 0.16862745 0.18823529]\n",
      "   [0.16078431 0.17254902 0.19215686]\n",
      "   [0.16078431 0.17254902 0.19215686]]\n",
      "\n",
      "  [[0.63529412 0.4627451  0.41176471]\n",
      "   [0.63529412 0.4627451  0.41176471]\n",
      "   [0.63137255 0.45882353 0.40784314]\n",
      "   ...\n",
      "   [0.16862745 0.18039216 0.2       ]\n",
      "   [0.16078431 0.17254902 0.19215686]\n",
      "   [0.16862745 0.18039216 0.2       ]]\n",
      "\n",
      "  [[0.6        0.43529412 0.38823529]\n",
      "   [0.60392157 0.43921569 0.39215686]\n",
      "   [0.61176471 0.44705882 0.4       ]\n",
      "   ...\n",
      "   [0.16862745 0.18039216 0.2       ]\n",
      "   [0.16470588 0.17647059 0.19607843]\n",
      "   [0.16862745 0.18039216 0.2       ]]]\n",
      "\n",
      "\n",
      " [[[0.18039216 0.18431373 0.20392157]\n",
      "   [0.18823529 0.2        0.21960784]\n",
      "   [0.18823529 0.20392157 0.21960784]\n",
      "   ...\n",
      "   [0.23921569 0.27058824 0.28235294]\n",
      "   [0.22745098 0.25882353 0.27058824]\n",
      "   [0.24313725 0.2745098  0.28627451]]\n",
      "\n",
      "  [[0.2        0.20392157 0.22352941]\n",
      "   [0.2        0.21176471 0.23137255]\n",
      "   [0.2        0.21960784 0.23529412]\n",
      "   ...\n",
      "   [0.23921569 0.27058824 0.28235294]\n",
      "   [0.24705882 0.27843137 0.29019608]\n",
      "   [0.23921569 0.27058824 0.28235294]]\n",
      "\n",
      "  [[0.20784314 0.21176471 0.23137255]\n",
      "   [0.20784314 0.21568627 0.23529412]\n",
      "   [0.2        0.21568627 0.23529412]\n",
      "   ...\n",
      "   [0.24705882 0.28235294 0.29019608]\n",
      "   [0.24705882 0.27843137 0.29019608]\n",
      "   [0.23921569 0.27058824 0.28235294]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.71764706 0.5254902  0.47058824]\n",
      "   [0.70980392 0.51764706 0.4627451 ]\n",
      "   [0.70980392 0.52156863 0.46666667]\n",
      "   ...\n",
      "   [0.16862745 0.18039216 0.2       ]\n",
      "   [0.17254902 0.18431373 0.20392157]\n",
      "   [0.16470588 0.17647059 0.19607843]]\n",
      "\n",
      "  [[0.68235294 0.49803922 0.45098039]\n",
      "   [0.67058824 0.49019608 0.43921569]\n",
      "   [0.67058824 0.48627451 0.43529412]\n",
      "   ...\n",
      "   [0.16862745 0.18039216 0.2       ]\n",
      "   [0.16470588 0.17647059 0.19607843]\n",
      "   [0.16078431 0.17254902 0.19215686]]\n",
      "\n",
      "  [[0.65098039 0.47843137 0.43529412]\n",
      "   [0.62352941 0.44705882 0.40392157]\n",
      "   [0.64705882 0.47058824 0.42745098]\n",
      "   ...\n",
      "   [0.16470588 0.17647059 0.19607843]\n",
      "   [0.16078431 0.17254902 0.19215686]\n",
      "   [0.17254902 0.18431373 0.20392157]]]\n",
      "\n",
      "\n",
      " [[[0.16470588 0.16862745 0.18823529]\n",
      "   [0.16470588 0.16862745 0.18823529]\n",
      "   [0.18039216 0.18431373 0.20392157]\n",
      "   ...\n",
      "   [0.18823529 0.19607843 0.24313725]\n",
      "   [0.18431373 0.19215686 0.23921569]\n",
      "   [0.18823529 0.19607843 0.24313725]]\n",
      "\n",
      "  [[0.16470588 0.16862745 0.18823529]\n",
      "   [0.16470588 0.16862745 0.18823529]\n",
      "   [0.16862745 0.17254902 0.19215686]\n",
      "   ...\n",
      "   [0.17647059 0.18431373 0.22352941]\n",
      "   [0.18431373 0.19607843 0.23529412]\n",
      "   [0.19215686 0.2        0.23921569]]\n",
      "\n",
      "  [[0.15294118 0.15686275 0.17647059]\n",
      "   [0.15686275 0.16078431 0.18039216]\n",
      "   [0.16470588 0.16862745 0.18823529]\n",
      "   ...\n",
      "   [0.17254902 0.18431373 0.21176471]\n",
      "   [0.18431373 0.19607843 0.22352941]\n",
      "   [0.19215686 0.20392157 0.23137255]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.08627451 0.09019608 0.10588235]\n",
      "   [0.08627451 0.09019608 0.10588235]\n",
      "   [0.07843137 0.08235294 0.09803922]\n",
      "   ...\n",
      "   [0.14901961 0.16078431 0.18039216]\n",
      "   [0.14509804 0.15686275 0.17647059]\n",
      "   [0.16078431 0.17254902 0.19215686]]\n",
      "\n",
      "  [[0.09411765 0.09803922 0.11372549]\n",
      "   [0.09411765 0.09803922 0.11372549]\n",
      "   [0.09019608 0.09411765 0.10980392]\n",
      "   ...\n",
      "   [0.14509804 0.15686275 0.17647059]\n",
      "   [0.14901961 0.16078431 0.18039216]\n",
      "   [0.14901961 0.16078431 0.18039216]]\n",
      "\n",
      "  [[0.09411765 0.09803922 0.11372549]\n",
      "   [0.09411765 0.09803922 0.11372549]\n",
      "   [0.09411765 0.09803922 0.11372549]\n",
      "   ...\n",
      "   [0.15294118 0.16470588 0.18431373]\n",
      "   [0.15686275 0.16862745 0.18823529]\n",
      "   [0.15686275 0.16862745 0.18823529]]]\n",
      "\n",
      "\n",
      " ...\n",
      "\n",
      "\n",
      " [[[0.03137255 0.04313725 0.0627451 ]\n",
      "   [0.03921569 0.05098039 0.07058824]\n",
      "   [0.04313725 0.05490196 0.0745098 ]\n",
      "   ...\n",
      "   [0.09411765 0.10588235 0.1254902 ]\n",
      "   [0.09803922 0.10980392 0.12941176]\n",
      "   [0.09803922 0.10980392 0.12941176]]\n",
      "\n",
      "  [[0.03137255 0.04313725 0.0627451 ]\n",
      "   [0.03921569 0.05098039 0.07058824]\n",
      "   [0.05098039 0.0627451  0.08235294]\n",
      "   ...\n",
      "   [0.09019608 0.10196078 0.12156863]\n",
      "   [0.09019608 0.10196078 0.12156863]\n",
      "   [0.09019608 0.10196078 0.12156863]]\n",
      "\n",
      "  [[0.03137255 0.04313725 0.0627451 ]\n",
      "   [0.03137255 0.04313725 0.0627451 ]\n",
      "   [0.05490196 0.06666667 0.08627451]\n",
      "   ...\n",
      "   [0.08627451 0.09803922 0.11764706]\n",
      "   [0.08627451 0.09803922 0.11764706]\n",
      "   [0.09019608 0.10196078 0.12156863]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.0745098  0.09019608 0.09411765]\n",
      "   [0.06666667 0.08235294 0.08627451]\n",
      "   [0.07058824 0.08627451 0.09019608]\n",
      "   ...\n",
      "   [0.06666667 0.07058824 0.08627451]\n",
      "   [0.08235294 0.08627451 0.10196078]\n",
      "   [0.08235294 0.08627451 0.10196078]]\n",
      "\n",
      "  [[0.05882353 0.0745098  0.07843137]\n",
      "   [0.05490196 0.07058824 0.0745098 ]\n",
      "   [0.05490196 0.07058824 0.0745098 ]\n",
      "   ...\n",
      "   [0.06666667 0.07058824 0.08627451]\n",
      "   [0.08235294 0.08627451 0.10196078]\n",
      "   [0.07843137 0.08235294 0.09803922]]\n",
      "\n",
      "  [[0.05882353 0.0745098  0.07843137]\n",
      "   [0.05490196 0.07058824 0.0745098 ]\n",
      "   [0.05882353 0.0745098  0.07843137]\n",
      "   ...\n",
      "   [0.0627451  0.06666667 0.08235294]\n",
      "   [0.07843137 0.08235294 0.09803922]\n",
      "   [0.0745098  0.07843137 0.09411765]]]\n",
      "\n",
      "\n",
      " [[[0.22745098 0.2627451  0.30196078]\n",
      "   [0.21960784 0.25490196 0.29803922]\n",
      "   [0.21176471 0.25098039 0.29019608]\n",
      "   ...\n",
      "   [0.09411765 0.10196078 0.14901961]\n",
      "   [0.11764706 0.1254902  0.17254902]\n",
      "   [0.10588235 0.11372549 0.16078431]]\n",
      "\n",
      "  [[0.22745098 0.25882353 0.30196078]\n",
      "   [0.21568627 0.24705882 0.29019608]\n",
      "   [0.20392157 0.23529412 0.27843137]\n",
      "   ...\n",
      "   [0.13333333 0.14117647 0.18823529]\n",
      "   [0.12941176 0.1372549  0.18431373]\n",
      "   [0.13333333 0.14117647 0.18823529]]\n",
      "\n",
      "  [[0.23921569 0.25882353 0.30588235]\n",
      "   [0.22745098 0.25098039 0.29803922]\n",
      "   [0.21960784 0.23921569 0.28627451]\n",
      "   ...\n",
      "   [0.14509804 0.15294118 0.2       ]\n",
      "   [0.14117647 0.14901961 0.19607843]\n",
      "   [0.13333333 0.14117647 0.18823529]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.22745098 0.18039216 0.19215686]\n",
      "   [0.12941176 0.09411765 0.10980392]\n",
      "   [0.08235294 0.0627451  0.09019608]\n",
      "   ...\n",
      "   [0.07058824 0.07843137 0.1254902 ]\n",
      "   [0.0745098  0.07843137 0.12941176]\n",
      "   [0.07058824 0.07843137 0.1254902 ]]\n",
      "\n",
      "  [[0.0627451  0.06666667 0.10588235]\n",
      "   [0.05882353 0.06666667 0.10588235]\n",
      "   [0.05882353 0.06666667 0.10980392]\n",
      "   ...\n",
      "   [0.0745098  0.08235294 0.12941176]\n",
      "   [0.07058824 0.07843137 0.1254902 ]\n",
      "   [0.0745098  0.08235294 0.12941176]]\n",
      "\n",
      "  [[0.0627451  0.05882353 0.10980392]\n",
      "   [0.0627451  0.05882353 0.10588235]\n",
      "   [0.07058824 0.05490196 0.09803922]\n",
      "   ...\n",
      "   [0.0745098  0.08235294 0.12941176]\n",
      "   [0.06666667 0.0745098  0.12156863]\n",
      "   [0.06666667 0.0745098  0.12156863]]]\n",
      "\n",
      "\n",
      " [[[0.19215686 0.24313725 0.27843137]\n",
      "   [0.19607843 0.24705882 0.28235294]\n",
      "   [0.19607843 0.24705882 0.28235294]\n",
      "   ...\n",
      "   [0.18039216 0.20784314 0.23921569]\n",
      "   [0.19607843 0.22352941 0.25490196]\n",
      "   [0.18823529 0.21568627 0.24705882]]\n",
      "\n",
      "  [[0.17254902 0.22352941 0.25882353]\n",
      "   [0.17647059 0.22745098 0.2627451 ]\n",
      "   [0.17647059 0.22745098 0.2627451 ]\n",
      "   ...\n",
      "   [0.18823529 0.21568627 0.24705882]\n",
      "   [0.19215686 0.21960784 0.25098039]\n",
      "   [0.18823529 0.21568627 0.24705882]]\n",
      "\n",
      "  [[0.15686275 0.20784314 0.24313725]\n",
      "   [0.15686275 0.20784314 0.24313725]\n",
      "   [0.16470588 0.21568627 0.25098039]\n",
      "   ...\n",
      "   [0.19215686 0.21960784 0.25098039]\n",
      "   [0.18823529 0.21568627 0.24705882]\n",
      "   [0.18823529 0.21568627 0.24705882]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.18039216 0.16862745 0.16078431]\n",
      "   [0.18823529 0.17254902 0.16470588]\n",
      "   [0.21176471 0.18039216 0.18039216]\n",
      "   ...\n",
      "   [0.16862745 0.18823529 0.20392157]\n",
      "   [0.16862745 0.18823529 0.20392157]\n",
      "   [0.16862745 0.18823529 0.20392157]]\n",
      "\n",
      "  [[0.13333333 0.15294118 0.16470588]\n",
      "   [0.13333333 0.14509804 0.15686275]\n",
      "   [0.1254902  0.12941176 0.14509804]\n",
      "   ...\n",
      "   [0.17647059 0.19607843 0.21176471]\n",
      "   [0.18039216 0.2        0.21568627]\n",
      "   [0.17647059 0.19607843 0.21176471]]\n",
      "\n",
      "  [[0.11764706 0.14509804 0.15294118]\n",
      "   [0.11764706 0.1372549  0.14901961]\n",
      "   [0.10980392 0.11764706 0.13333333]\n",
      "   ...\n",
      "   [0.2        0.21960784 0.23529412]\n",
      "   [0.19215686 0.21176471 0.22745098]\n",
      "   [0.18431373 0.20392157 0.21960784]]]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape:  (30771, 64, 64, 3)\n",
      "Y shape:  (30771, 3)\n",
      "WARNING:tensorflow:From /Users/yunho/.pyenv/versions/3.7.4/envs/Tobigs_2018-env/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:58: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/yunho/.pyenv/versions/3.7.4/envs/Tobigs_2018-env/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:442: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/yunho/.pyenv/versions/3.7.4/envs/Tobigs_2018-env/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3543: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/yunho/.pyenv/versions/3.7.4/envs/Tobigs_2018-env/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3386: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/yunho/.pyenv/versions/3.7.4/envs/Tobigs_2018-env/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3388: The name tf.nn.avg_pool is deprecated. Please use tf.nn.avg_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/yunho/.pyenv/versions/3.7.4/envs/Tobigs_2018-env/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:1205: calling reduce_prod_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From /Users/yunho/.pyenv/versions/3.7.4/envs/Tobigs_2018-env/lib/python3.7/site-packages/keras/optimizers.py:711: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/yunho/.pyenv/versions/3.7.4/envs/Tobigs_2018-env/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:2755: calling reduce_sum_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From /Users/yunho/.pyenv/versions/3.7.4/envs/Tobigs_2018-env/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:2759: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/yunho/.pyenv/versions/3.7.4/envs/Tobigs_2018-env/lib/python3.7/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /Users/yunho/.pyenv/versions/3.7.4/envs/Tobigs_2018-env/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:899: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/yunho/.pyenv/versions/3.7.4/envs/Tobigs_2018-env/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:625: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /Users/yunho/.pyenv/versions/3.7.4/envs/Tobigs_2018-env/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:886: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/yunho/.pyenv/versions/3.7.4/envs/Tobigs_2018-env/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:2294: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "Train on 30771 samples, validate on 3420 samples\n",
      "Epoch 1/5\n",
      "WARNING:tensorflow:From /Users/yunho/.pyenv/versions/3.7.4/envs/Tobigs_2018-env/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:153: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/yunho/.pyenv/versions/3.7.4/envs/Tobigs_2018-env/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:158: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/yunho/.pyenv/versions/3.7.4/envs/Tobigs_2018-env/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:333: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/yunho/.pyenv/versions/3.7.4/envs/Tobigs_2018-env/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:341: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "30771/30771 [==============================] - 2223s - loss: 0.0816 - acc: 0.9795 - val_loss: 0.1634 - val_acc: 0.9520\n",
      "Epoch 2/5\n",
      "30771/30771 [==============================] - 2112s - loss: 0.0070 - acc: 0.9985 - val_loss: 0.0182 - val_acc: 0.9953\n",
      "Epoch 3/5\n",
      "30771/30771 [==============================] - 2138s - loss: 0.0039 - acc: 0.9989 - val_loss: 1.6288e-05 - val_acc: 1.0000\n",
      "Epoch 4/5\n",
      "30771/30771 [==============================] - 2205s - loss: 0.0165 - acc: 0.9971 - val_loss: 0.0450 - val_acc: 0.9930\n",
      "Epoch 5/5\n",
      "30771/30771 [==============================] - 2151s - loss: 0.0046 - acc: 0.9989 - val_loss: 1.0359e-04 - val_acc: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1397d3610>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset, Y = load_preprocessed_data(path=train_path)\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(dataset, Y, test_size=0.1, stratify=Y, random_state=0)\n",
    "\n",
    "print(\"X shape: \", X_train.shape)\n",
    "print(\"Y shape: \", Y_train.shape)\n",
    "\n",
    "Resnet = CNN(in_shape=(64, 64, 3), classes=3)\n",
    "Resnet.fit(X_train, Y_train, batch_size=32, epochs=5, validation_data=(X_val, Y_val), callbacks=[EarlyStopping(patience=10)])\n",
    "\n",
    "\n",
    "# result : validation accuracy - 100% after 5th epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape:  (70, 64, 64, 3)\n",
      "Y shape:  (70, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test, Y_test = load_preprocessed_data(path=test_path)\n",
    "print(\"X shape: \", X_test.shape)\n",
    "print(\"Y shape: \", Y_test.shape)\n",
    "Y_predicted = Resnet.predict(X_test, batch_size=32)\n",
    "P = np.argmax(Y_predicted, axis=1)\n",
    "T = np.argmax(Y_test, axis=1)\n",
    "np.sum(P == T) / 70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
